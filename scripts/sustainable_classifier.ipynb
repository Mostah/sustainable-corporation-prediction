{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sustainable-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4go4QEfAaucb"
      },
      "source": [
        "import pandas as pd\r\n",
        "import re\r\n",
        "\r\n",
        "from copy import deepcopy\r\n",
        "from itertools import chain\r\n",
        "from math import ceil\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "from transformers import RobertaModel\r\n",
        "from transformers import RobertaTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "070HuVT1bes0",
        "outputId": "304b44fe-ecf7-4822-d21f-f9cac7048fcf"
      },
      "source": [
        "import pickle5 as pickle\r\n",
        "\r\n",
        "df = None\r\n",
        "with open('./sustainalytics_dataset.pkl', 'rb') as fh:\r\n",
        "  df = pickle.load(fh)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>rating</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1&amp;1 Drillisch AG</td>\n",
              "      <td>21.1</td>\n",
              "      <td>1&amp;1 Drillisch AG (formerly known as: Drillisch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2i Rete Gas SpA</td>\n",
              "      <td>37.2</td>\n",
              "      <td>Headquartered in Milan, 2i Rete Gas  is the se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2U, Inc.</td>\n",
              "      <td>15.7</td>\n",
              "      <td>2U, Inc. (formerly 2tor Inc.) is an American e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3i Group PLC</td>\n",
              "      <td>12.6</td>\n",
              "      <td>3i Group plc is a British multinational privat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3i Infrastructure PLC</td>\n",
              "      <td>22.1</td>\n",
              "      <td>3i Infrastructure plc (LSE: 3IN) is an investm...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    name  ...                                            content\n",
              "0       1&1 Drillisch AG  ...  1&1 Drillisch AG (formerly known as: Drillisch...\n",
              "1        2i Rete Gas SpA  ...  Headquartered in Milan, 2i Rete Gas  is the se...\n",
              "2               2U, Inc.  ...  2U, Inc. (formerly 2tor Inc.) is an American e...\n",
              "4           3i Group PLC  ...  3i Group plc is a British multinational privat...\n",
              "5  3i Infrastructure PLC  ...  3i Infrastructure plc (LSE: 3IN) is an investm...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zZJuVyRTIll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08193ef8-2def-4769-d0ab-df2c9abfdcf8"
      },
      "source": [
        "df = df[df['content'].apply(lambda content: len(content.split(' '))) <= 5500]\r\n",
        "df = df[df['content'].apply(lambda content: len(content.split(' '))) >= 400]\r\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T6fhm87bijz",
        "outputId": "337eec46-220a-4398-96b9-408f1a8fd056"
      },
      "source": [
        "def clean_text(content):\r\n",
        "    content = re.sub('\\n', ' ', content)\r\n",
        "    content = re.sub('[=]{2,}', ' ', content)\r\n",
        "    content = re.sub('[ ]{2,}', ' ', content)\r\n",
        "    return content\r\n",
        "\r\n",
        "df['content'] = df['content'].apply(clean_text)\r\n",
        "\r\n",
        "X = list(df['content'])\r\n",
        "y = list(df['rating'])\r\n",
        "\r\n",
        "print(len(X))\r\n",
        "\r\n",
        "print(min(y))\r\n",
        "print(max(y))\r\n",
        "print(sum(y)/len(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2100\n",
            "7.2\n",
            "67.4\n",
            "24.88809523809527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkK0YQLAbpqm",
        "outputId": "e1e705d0-8721-437e-c42d-6e2db65aedb1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\r\n",
        "print(len(X_train))\r\n",
        "print(len(X_test))\r\n",
        "\r\n",
        "# Nombre moyens de mots par article wiki\r\n",
        "print(df['content'].apply(lambda content: len(content.split(' '))).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1890\n",
            "210\n",
            "1544.3685714285714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o8_-DuH6ren"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "y_train = scaler.fit_transform(np.array(y_train, dtype=float).reshape(-1, 1))\r\n",
        "y_train = y_train.reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EUDjxCFdGPv"
      },
      "source": [
        "max_length = 200\r\n",
        "overlap_length = 50\r\n",
        "\r\n",
        "def split_tokens_sequence(tokens_sequence):\r\n",
        "    n = len(tokens_sequence)\r\n",
        "    sequences = list()\r\n",
        "    \r\n",
        "    first_chunk = torch.tensor(tokens_sequence[0 : min(max_length, n)])\r\n",
        "    if max_length > n:\r\n",
        "        first_chunk = nn.functional.pad(first_chunk, (0, max_length - n))\r\n",
        "    sequences.append(first_chunk)\r\n",
        "    \r\n",
        "    for i in range(ceil((n - max_length) / (max_length - overlap_length))):\r\n",
        "        initial_pos = (max_length - overlap_length)*(i + 1)\r\n",
        "        final_pos = min((max_length - overlap_length)*(i + 1) + max_length, n)\r\n",
        "        sequences.append(torch.tensor(tokens_sequence[initial_pos : final_pos]))\r\n",
        "    \r\n",
        "    sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\r\n",
        "    \r\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmBdk_nWdI8u"
      },
      "source": [
        "class Model:\r\n",
        "\r\n",
        "    def __init__(self, max_items_per_batch, scaler):\r\n",
        "        self.max_items_per_batch = max_items_per_batch\r\n",
        "        self.scaler = scaler\r\n",
        "\r\n",
        "        self.model = RobertaModel.from_pretrained('roberta-base')\r\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\r\n",
        "\r\n",
        "        self.lstm = nn.LSTM(input_size=768, hidden_size=1, num_layers=1)\r\n",
        "        \r\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "        self.model = self.model.to(self.device)\r\n",
        "        self.lstm = self.lstm.to(self.device)\r\n",
        "\r\n",
        "        print(f\"Running model on {self.device}\")\r\n",
        "    \r\n",
        "\r\n",
        "    def forward(self, sequences_lengths, input_ids_sequences, attention_masks_sequences=None, labels=None):\r\n",
        "        \"\"\"Compute forward propagation\r\n",
        "\r\n",
        "        Args:\r\n",
        "            sequences_lengths (List[int]): List of the lengths of each \r\n",
        "                sequence\r\n",
        "            input_ids_sequences (Tensor): Tensor with the list of the list of \r\n",
        "                each tokens sequence\r\n",
        "            attention_masks_sequences (Tensor, optional): Tensor with the list \r\n",
        "                of the list of each attention masks sequence. \r\n",
        "                Defaults to None.\r\n",
        "            labels (Tensor, optional): Tensor with the list of labels. \r\n",
        "                Defaults to None.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            Tensor: If the labels are specified, returns the loss; otherwise, \r\n",
        "                returns the predicted outputs\r\n",
        "        \"\"\"\r\n",
        "        outputs = self.model(input_ids_sequences, attention_masks_sequences)['pooler_output']\r\n",
        "\r\n",
        "        max_sequence_length = max(sequences_lengths)\r\n",
        "        outputs_grouped = list()\r\n",
        "        index = 0\r\n",
        "        for length in sequences_lengths:\r\n",
        "            outputs_grouped.append( \r\n",
        "                nn.functional.pad(outputs[index:index+length,:], \r\n",
        "                                  (0, 0, max_sequence_length - length, 0))\r\n",
        "            )\r\n",
        "            index += length\r\n",
        "        lstm_inputs = nn.utils.rnn.pad_sequence(outputs_grouped, batch_first=True)\r\n",
        "\r\n",
        "        outputs = torch.flatten(self.lstm(lstm_inputs)[0][:,-1,:])\r\n",
        "\r\n",
        "        if labels is not None:\r\n",
        "            loss_fct = nn.MSELoss()\r\n",
        "            loss = loss_fct(outputs.float(), labels.float())\r\n",
        "            return loss\r\n",
        "        else:\r\n",
        "            return outputs\r\n",
        "    \r\n",
        "\r\n",
        "    def predict(self, X, y=None):\r\n",
        "        \"\"\"Predict the outputs of a batch of inputs\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (List[str]): The list of strings wich outputs to predict. \r\n",
        "                If one of the entries is too long to be handled, raises a \r\n",
        "                RuntimeError, except when the list of labels `y` is known and \r\n",
        "                given to the function\r\n",
        "            y (List[float], optional): The list of labels. If specified, \r\n",
        "                ables the model to remove the not handable entries, and \r\n",
        "                returns the labels related to the handable entries. \r\n",
        "                Defaults to None.\r\n",
        "\r\n",
        "        Raises:\r\n",
        "            RuntimeError: If one element of `X` is too long to be handled \r\n",
        "                (the number of chunks of tokens is too high, above \r\n",
        "                `self.max_items_per_batch`), raises a RuntimeError\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            Tensor, Optional[List[float]]: Returns the predicted Tensor. \r\n",
        "                If `y` is given, returns also the list of labels from which \r\n",
        "                are removed all the not handable entries.\r\n",
        "        \"\"\"\r\n",
        "        if y is not None:\r\n",
        "            y_copy = deepcopy(y)\r\n",
        "        \r\n",
        "        self.model.eval()\r\n",
        "\r\n",
        "        input_ids = self.tokenizer(X)['input_ids']\r\n",
        "        list_tokens_sequences = list(map(split_tokens_sequence, input_ids))\r\n",
        "\r\n",
        "        batches = list()\r\n",
        "        batch = list()\r\n",
        "        sequences_lengths = list()\r\n",
        "\r\n",
        "        indexes_to_pop = list()\r\n",
        "        for index, tokens_sequence in enumerate(list_tokens_sequences):\r\n",
        "            \r\n",
        "            if len(tokens_sequence) > self.max_items_per_batch:\r\n",
        "                if y is None:\r\n",
        "                    raise RuntimeError(f\"The sequence must be shorter than {self.max_items_per_batch} chunks\")\r\n",
        "                indexes_to_pop.append(index)\r\n",
        "\r\n",
        "                continue\r\n",
        "            \r\n",
        "            if sum(sequences_lengths) + len(tokens_sequence) <= self.max_items_per_batch:\r\n",
        "                batch.append(tokens_sequence)\r\n",
        "                sequences_lengths.append(len(tokens_sequence))\r\n",
        "            \r\n",
        "            else:\r\n",
        "                batches.append((batch, sequences_lengths))\r\n",
        "\r\n",
        "                batch = [tokens_sequence]\r\n",
        "                sequences_lengths = [len(tokens_sequence)]\r\n",
        "        \r\n",
        "        for index in reversed(indexes_to_pop):\r\n",
        "            y_copy.pop(index)\r\n",
        "\r\n",
        "        if len(batch) > 0:\r\n",
        "            batches.append((batch, sequences_lengths))\r\n",
        "        \r\n",
        "        outputs = list()\r\n",
        "        with torch.no_grad():\r\n",
        "            for list_tokens_sequences, sequences_lengths in batches:\r\n",
        "                input_ids_sequences = torch.cat(list_tokens_sequences).to(self.device)\r\n",
        "                outputs.append(self.forward(sequences_lengths, input_ids_sequences))\r\n",
        "        \r\n",
        "        if y is None:\r\n",
        "            return torch.cat(outputs)\r\n",
        "        \r\n",
        "        return torch.cat(outputs), y_copy\r\n",
        "    \r\n",
        "\r\n",
        "    def evaluate(self, X, y, normalized_y=False, return_random_error=True):\r\n",
        "        \"\"\"Compute the average prediction error of the model, given a \r\n",
        "        test dataset\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (List[str]): List of inputs\r\n",
        "            y (List[float]): List of outputs\r\n",
        "            normalized_y (bool, optional): Specifies if `y` variables have \r\n",
        "                been normalized. Defaults to False.\r\n",
        "            return_random_error (bool, optional): If `True`, computes the \r\n",
        "                random error, which corresponds to the default error if the \r\n",
        "                model always returned the average value. Defaults to True.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            float, Optional[float]: Returns the average prediction error. \r\n",
        "                If `return_random_error` is `True`, also returns the random\r\n",
        "                average error.\r\n",
        "        \"\"\"\r\n",
        "        outputs, y = self.predict(X, y)\r\n",
        "        outputs = outputs.cpu().numpy()\r\n",
        "        outputs = self.scaler.inverse_transform(outputs)\r\n",
        "\r\n",
        "        if normalized_y:\r\n",
        "            y = self.scaler.inverse_transform(y)\r\n",
        "\r\n",
        "        mean_error = np.abs(outputs - np.array(y)).mean()\r\n",
        "\r\n",
        "        if return_random_error:\r\n",
        "            random_error = np.abs(scaler.mean_ - np.array(y)).mean()\r\n",
        "            return mean_error, random_error\r\n",
        "        \r\n",
        "        return mean_error\r\n",
        "    \r\n",
        "\r\n",
        "    def freeze_encoder(self):\r\n",
        "        for param in self.model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "    \r\n",
        "\r\n",
        "    def unfreeze_encoder(self):\r\n",
        "        for param in self.model.parameters():\r\n",
        "            param.requires_grad = True\r\n",
        "    \r\n",
        "\r\n",
        "    def train(self, X, y, n_epochs, max_batch_size, lr_init, lr_max, n_epochs_finetuning=0, X_test=None, y_test=None):\r\n",
        "        \"\"\"Train the model on a given dataset.\r\n",
        "            * Randomly order the samples at the beginning of each epoch\r\n",
        "            * Apply the \"triangular learning rates\" policy: computes a list \r\n",
        "              of adaptative learning rates, first linearly increasing \r\n",
        "              from `lr_init` to `lr_max`, then linearly decreasing \r\n",
        "              back to `lr_init`\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (List[str]): List of inputs\r\n",
        "            y (List[float]): List of outputs\r\n",
        "            n_epochs (int): Number of epochs\r\n",
        "            max_batch_size (int): Maximum size of a batch (due to the specific\r\n",
        "                model, the size of the batch is not constant, but \r\n",
        "                `max_batch_size` specifies its maximum value)\r\n",
        "            lr_init (float): Initial value of the learning rate\r\n",
        "            lr_max (float): Maximal value of the learning rate\r\n",
        "            X_test (List[str], optional): List of test inputs.\r\n",
        "                If not `None` and `y_test` not `None`, at the end of each \r\n",
        "                epoch, computes the evaluation of the model on this test \r\n",
        "                dataset. Defaults to None.\r\n",
        "            y_test (List[float], optional): List of test outputs.\r\n",
        "                If not `None` and `X_test` not `None`, at the end of each \r\n",
        "                epoch, computes the evaluation of the model on this test \r\n",
        "                dataset. Defaults to None.\r\n",
        "        \"\"\"\r\n",
        "        rise = np.linspace(lr_init, lr_max, ceil(n_epochs / 2))\r\n",
        "        lrs = np.concatenate((rise, np.flip(rise)[1:], \r\n",
        "                              np.array([lr_init])))[:n_epochs]\r\n",
        "\r\n",
        "        if X_test is not None and y_test is not None:\r\n",
        "            print(f\"\\nInit, evaluation: {self.evaluate(X_test, y_test)}\")\r\n",
        "\r\n",
        "        # Fine-tuning the superficial layers\r\n",
        "        self.freeze_encoder()\r\n",
        "\r\n",
        "        optimizer = torch.optim.AdamW(self.lstm.parameters(), \r\n",
        "                                      lr=lr_init)\r\n",
        "\r\n",
        "        for epoch in range(n_epochs_finetuning):\r\n",
        "            indexes_order = np.random.choice(range(len(X)), \r\n",
        "                                             len(X), \r\n",
        "                                             replace=False)\r\n",
        "            list_tokens_sequences = list()\r\n",
        "            list_attention_masks = list()\r\n",
        "            list_labels = list()\r\n",
        "            sequences_lengths = list()\r\n",
        "\r\n",
        "            for index in tqdm(indexes_order):\r\n",
        "\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                tokenized = self.tokenizer(X[index])\r\n",
        "                input_ids, attention_masks = tokenized['input_ids'], tokenized['attention_mask']\r\n",
        "                 \r\n",
        "                tokens_sequences = split_tokens_sequence(input_ids)\r\n",
        "                attention_masks_sequences = split_tokens_sequence(attention_masks)\r\n",
        "\r\n",
        "                # Ignore the item\r\n",
        "                if len(tokens_sequences) > self.max_items_per_batch:\r\n",
        "                    continue\r\n",
        "                \r\n",
        "                # Run the batch then create a new one\r\n",
        "                if len(sequences_lengths) == max_batch_size or sum(sequences_lengths) + len(tokens_sequences) > self.max_items_per_batch:\r\n",
        "                    input_ids_sequences = torch.cat(list_tokens_sequences).to(self.device)\r\n",
        "                    attention_masks = torch.cat(list_attention_masks).to(self.device)\r\n",
        "                    \r\n",
        "                    loss = self.forward(sequences_lengths, \r\n",
        "                                        input_ids_sequences, \r\n",
        "                                        attention_masks,\r\n",
        "                                        labels=torch.tensor(list_labels).to(self.device))\r\n",
        "\r\n",
        "                    loss.backward()\r\n",
        "                    optimizer.step()\r\n",
        "\r\n",
        "                    list_tokens_sequences = [tokens_sequences]\r\n",
        "                    list_attention_masks = [attention_masks_sequences]\r\n",
        "                    list_labels = [y[index]]\r\n",
        "                    sequences_lengths = [len(tokens_sequences)]\r\n",
        "                \r\n",
        "                # Add the current item to the batch\r\n",
        "                else:\r\n",
        "                    list_tokens_sequences.append(tokens_sequences)\r\n",
        "                    list_attention_masks.append(attention_masks_sequences)\r\n",
        "                    list_labels.append(y[index])\r\n",
        "                    sequences_lengths.append(len(tokens_sequences))\r\n",
        "\r\n",
        "            # Run last item    \r\n",
        "            input_ids_sequences = torch.cat(list_tokens_sequences).to(self.device)\r\n",
        "            attention_masks_sequences = torch.cat(list_attention_masks).to(self.device)\r\n",
        "            \r\n",
        "            loss = self.forward(sequences_lengths, \r\n",
        "                                input_ids_sequences, \r\n",
        "                                attention_masks_sequences,\r\n",
        "                                labels=torch.tensor(list_labels).to(self.device))\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "            if X_test is not None and y_test is not None:\r\n",
        "                print(f\"\\nEpoch: {epoch}, evaluation: {self.evaluate(X_test, y_test)}\")\r\n",
        "\r\n",
        "        # Training the whole model\r\n",
        "\r\n",
        "        self.unfreeze_encoder()\r\n",
        "        self.model.train()\r\n",
        "        optimizer = torch.optim.AdamW(chain(self.model.parameters(), \r\n",
        "                                            self.lstm.parameters()), \r\n",
        "                                      lr=lrs[0])\r\n",
        "\r\n",
        "        for epoch in range(n_epochs):\r\n",
        "\r\n",
        "            # Update the learning rate\r\n",
        "            if epoch > 0:\r\n",
        "                print(f\"LR: {lrs[epoch]}\")\r\n",
        "                for param_group in optimizer.param_groups:\r\n",
        "                    param_group['lr'] = lrs[epoch]\r\n",
        "\r\n",
        "            indexes_order = np.random.choice(range(len(X)), \r\n",
        "                                             len(X), \r\n",
        "                                             replace=False)\r\n",
        "            list_tokens_sequences = list()\r\n",
        "            list_attention_masks = list()\r\n",
        "            list_labels = list()\r\n",
        "            sequences_lengths = list()\r\n",
        "\r\n",
        "            for index in tqdm(indexes_order):\r\n",
        "\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                tokenized = self.tokenizer(X[index])\r\n",
        "                input_ids, attention_masks = tokenized['input_ids'], tokenized['attention_mask']\r\n",
        "                 \r\n",
        "                tokens_sequences = split_tokens_sequence(input_ids)\r\n",
        "                attention_masks_sequences = split_tokens_sequence(attention_masks)\r\n",
        "\r\n",
        "                # Ignore the item\r\n",
        "                if len(tokens_sequences) > self.max_items_per_batch:\r\n",
        "                    continue\r\n",
        "                \r\n",
        "                # Run the batch then create a new one\r\n",
        "                if len(sequences_lengths) == max_batch_size or sum(sequences_lengths) + len(tokens_sequences) > self.max_items_per_batch:\r\n",
        "                    input_ids_sequences = torch.cat(list_tokens_sequences).to(self.device)\r\n",
        "                    attention_masks = torch.cat(list_attention_masks).to(self.device)\r\n",
        "                    \r\n",
        "                    loss = self.forward(sequences_lengths, \r\n",
        "                                        input_ids_sequences, \r\n",
        "                                        attention_masks,\r\n",
        "                                        labels=torch.tensor(list_labels).to(self.device))\r\n",
        "\r\n",
        "                    loss.backward()\r\n",
        "                    optimizer.step()\r\n",
        "\r\n",
        "                    list_tokens_sequences = [tokens_sequences]\r\n",
        "                    list_attention_masks = [attention_masks_sequences]\r\n",
        "                    list_labels = [y[index]]\r\n",
        "                    sequences_lengths = [len(tokens_sequences)]\r\n",
        "                \r\n",
        "                # Add the current item to the batch\r\n",
        "                else:\r\n",
        "                    list_tokens_sequences.append(tokens_sequences)\r\n",
        "                    list_attention_masks.append(attention_masks_sequences)\r\n",
        "                    list_labels.append(y[index])\r\n",
        "                    sequences_lengths.append(len(tokens_sequences))\r\n",
        "\r\n",
        "            # Run last item    \r\n",
        "            input_ids_sequences = torch.cat(list_tokens_sequences).to(self.device)\r\n",
        "            attention_masks_sequences = torch.cat(list_attention_masks).to(self.device)\r\n",
        "            \r\n",
        "            loss = self.forward(sequences_lengths, \r\n",
        "                                input_ids_sequences, \r\n",
        "                                attention_masks_sequences,\r\n",
        "                                labels=torch.tensor(list_labels).to(self.device))\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            print(f\"Loss: {loss.item()}\")\r\n",
        "            \r\n",
        "            if X_test is not None and y_test is not None:\r\n",
        "                print(f\"\\nEpoch: {epoch}, evaluation: {self.evaluate(X_test, y_test)}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GZIyLo2dMoG",
        "outputId": "38c5bef8-78e0-4264-93f0-c9a807a6c580"
      },
      "source": [
        "max_items_per_batch = 40\r\n",
        "model = Model(max_items_per_batch, scaler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running model on cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPPOAJoLgjPk",
        "outputId": "a40ff93c-c5cc-4e8b-f4e1-42731305f7c0"
      },
      "source": [
        "model.train(X_train, y_train, n_epochs=2, max_batch_size=3, \r\n",
        "            lr_init=1e-5, lr_max=1e-4, n_epochs_finetuning=0,\r\n",
        "            X_test=X_test, y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2019 > 512). Running this sequence through the model will result in indexing errors\n",
            "  0%|          | 0/1890 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Init, evaluation: (7.085056528230993, 7.067837398373984)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1890/1890 [12:39<00:00,  2.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.5052616596221924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1890 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0, evaluation: (7.067838836297756, 7.067837398373984)\n",
            "LR: 1e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1890/1890 [12:26<00:00,  2.53it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.7577447891235352\n",
            "\n",
            "Epoch: 1, evaluation: (7.067837477893364, 7.067837398373984)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvYzbYeiArac",
        "outputId": "4ff849f1-151a-453a-8fbd-27c8beebed98"
      },
      "source": [
        "predictions = model.predict(X_test[:5])\r\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.4782e-18, 4.1742e-20, 6.6936e-22, 3.9714e-17, 1.4084e-21],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}